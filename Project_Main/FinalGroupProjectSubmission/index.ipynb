{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Overview"
   ],
   "metadata": {
    "id": "Kb8sO2HELy9z"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the git repo link: https://github.com/sreejeetsreenivasan/PIC16B-Project"
   ],
   "metadata": {
    "id": "SOXJ1UztMoI4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The problem we want to solve is understanding and managing the phenomena of delayed stops—places where there is already high ridership and any delay causes large disruptions. We will be focusing on the LA Metro and investigating delay source stations that are most likely to have a large impact on the stations we care about through the propagation of delays from that source station. This is important because delays can disrupt passenger schedules, affect overall network performance, and contribute to persistent negative impacts throughout the transportation system. Timely and reliable transit services are essential for fostering sustainable urban mobility, reducing congestion in the inner city, increasing economic output, and promoting public transportation as a viable alternative.\n",
    "\n",
    "We began with some large scale data cleaning, scraping, and collection as we planned to do some examples with other transit systems as well as that of Los Angeles. These included some testing examples with New York and Boston’s rapid transit systems. The project was initially more broad, but as we ran into constraints along the way, such as the lack of suitable data for certain transit systems, we narrowed our project to focus more on LA’s own rail system. We recognized that using either Scrapy or BeautifulSoup to parse the data directly from the website would be the most efficient way to retrieve the information. However, we found that the url of the linked website before did not change as we would submit new information in regard to the month and year for which we wanted data. As such, we realized that Selenium would be the most optimal way to work around this.\n",
    "\n",
    "Taking this data we obtained through GTFS and the data we scraped, we can use the package NetworkX to create graph objects of our rail network. With the stations as nodes and the routes as the edges, we are able to now perform mathematical analysis on the network. After some simple analysis, we understood that we did not have enough tools at our disposal to answer our questions and proceeded to see what methods other data scientists have employed. We narrowed our research down to 3 papers and using ideas from all three, we were able to improve the method we encountered earlier on in our exploration. The culmination of our analysis was in the creation of our implementation of the reverse localized pathing (RLP) method. Using the results from our function, we could then go ahead and visualize the results. \n",
    "\n"
   ],
   "metadata": {
    "id": "vEcF3CnEMuJT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. [First Technical Component: Sreejeet Scraping]\n",
    "\n",
    "We used tools that we had learned both during lecture and discussion to complete this task. Our approach followed a few key steps:\n",
    "\n",
    "1. Using the \"developer tools\" window in the Chrome browser, we were able to find the exact table and id tags which we needed in order to access that particular tabular group on the website\n",
    "2. We recognized that BeautifulSoup would be the most useful tool in scraping this information off. We considered using Scrapy at first, but BeautifulSoup got the job done without some of the implementation complexity we encountered with Scrapy\n",
    "3. We used the Python Selenium module to use the \"webdriver\" class provided by the module to help input the \"month\" and \"year\" fields in the dropdowns. We realized we needed this module because the data was interactive (i.e. the data loaded without changing the url), and using the Requests module was not particularly helpful, even when sending POST request data\n",
    "\n"
   ],
   "metadata": {
    "id": "RUkfAH-mLzJj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. [Second Technical Component: NetworkX and math]"
   ],
   "metadata": {
    "id": "lKuWrScFLzRp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. [Third Technical Component: Archer Visualziations]"
   ],
   "metadata": {
    "id": "BxdjSFJCLzkl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "3sIWRC6ZLzwI"
   }
  }
 ]
}
