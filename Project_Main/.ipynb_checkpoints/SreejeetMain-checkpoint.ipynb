{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550a1c24",
   "metadata": {},
   "source": [
    "# Introduction to Our Scraper\n",
    "\n",
    "For this project, we needed to scrape the data off of the metro data website (we only considered rail data), given at this link: https://isotp.metro.net/MetroRidership/YearOverYear.aspx\n",
    "\n",
    "We used tools that we had learned both during lecture and discussion to complete this task. Our approach followed a few key steps:\n",
    "\n",
    "1. Using the \"developer tools\" window in the Chrome browser, we were able to find the exact table and id tags which we needed in order to access that particular tabular group on the website\n",
    "2. We recognized that BeautifulSoup would be the most useful tool in scraping this information off. We considered using Scrapy at first, but BeautifulSoup got the job done without some of the implementation complexity we encountered with Scrapy\n",
    "3. We used the Python Selenium module to use the \"webdriver\" class provided by the module to help input the \"month\" and \"year\" fields in the dropdowns. We realized we needed this module because the data was interactive (i.e. the data loaded without changing the url), and using the Requests module was not particularly helpful, even when sending POST request data\n",
    "\n",
    "The whole script is implemented in the file \"scraping_script.py\" in the Project_Main directory\n",
    "\n",
    "Disclaimer: Opening the scripting file via Jupyter notebook may display it having certain indentation issues. This is because the script was entirely written using the Vim text editor through Terminal, not Jupyter notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1239dd",
   "metadata": {},
   "source": [
    "### Methods\n",
    "\n",
    "The two methods implemented in the file are \"parse_ridership\" and \"selenium_driver\". We can consider \"selenium_driver\" to essentially be a \"main()\" function, while \"parse_ridership\" is used in association with that function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44386987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium_driver implementation\n",
    "\n",
    "def selenium_driver(month: str, year: str, list_of_ids: list):\n",
    "    \"\"\"\n",
    "    This method takes in a month and year, and uses it as input to submit a form on the LA Metro ridership website\n",
    "    This returns a dataframe which contains the ridership information, as well as the weighted importance of each line\n",
    "    by year when possible\n",
    "\n",
    "    The LA Metro website stores the data for all three years including the year passed in (eg.\"2023\" will provide information\n",
    "    for 2022 and 2021 as well)\n",
    "    \"\"\"\n",
    "    # Preliminary input checking\n",
    "    if type(month) != str and type(year) != str:\n",
    "        return \"ValueError: Month and year must be a string\"\n",
    "    if int(year) >= 2024 or int(year) < 2009:\n",
    "        return \"Year must be between 2009 and 2023\"\n",
    "    if int(year) == 2023:\n",
    "        if month == \"November\" or month == \"December\":\n",
    "            return \"Month must be October or earlier for the year 2023\"\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(\"https://isotp.metro.net/MetroRidership/YearOverYear.aspx\")\n",
    "\n",
    "    # Month element\n",
    "    month_element_tag = driver.find_element(By.XPATH, \"//select[@name='ctl00$ContentPlaceHolder1$ddlPeriod']\")\n",
    "    month_options = month_element_tag.find_elements(By.TAG_NAME, 'option')\n",
    "\n",
    "    for option in month_options:\n",
    "        month_val = option.text\n",
    "        if month_val == month:\n",
    "            option.click()\n",
    "            break\n",
    "   \n",
    "    # Year element\n",
    "    year_element_tag = driver.find_element(By.XPATH, \"//select[@name='ctl00$ContentPlaceHolder1$ddlYear']\")\n",
    "    year_options = year_element_tag.find_elements(By.TAG_NAME, 'option')\n",
    "\n",
    "    for option in year_options:\n",
    "        year_val = option.text\n",
    "        if year_val == year:\n",
    "            option.click()\n",
    "            break\n",
    "   \n",
    "    # Navigate to submit button\n",
    "    driver.find_element(By.ID, \"ContentPlaceHolder1_btnSubmit\").click()\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    rider_dataframes = parse_ridership(list_of_ids, soup)\n",
    "\n",
    "    # Now rider dataframe has information about the ridership by line\n",
    "    # We can now use Pandas to manipulate this data and assign weights\n",
    "\n",
    "    # This gets just the total_ridership row, which is in the first dataframe in the rider_dataframe list\n",
    "    try:\n",
    "        sys_ridership = rider_dataframes[0].loc[\"Total Boardings\"].str.replace(\",\", \"\").astype(int)\n",
    "    except ValueError:\n",
    "        return f\"No ridership data for {month} {year}\"\n",
    "    # Now we have total ridership for the month for 3 years (i.e. 2023-2021)\n",
    "    # We want to consider the proportions for each of the other dataframes, and add this as another row\n",
    "\n",
    "    for df_index in range(1, len(rider_dataframes)):\n",
    "        try:\n",
    "            rider_dataframes[df_index].loc['Proportion of Total Boardings'] = rider_dataframes[df_index].loc['Total Boardings'].str.replace(\",\", \"\").astype(int) / sys_ridership\n",
    "        except ValueError as e:\n",
    "            pass\n",
    "    time.sleep(3)\n",
    "    driver.quit()\n",
    "    return rider_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965e6a88",
   "metadata": {},
   "source": [
    "### Overview of \"selenium_driver\" method\n",
    "\n",
    "The driver takes in 3 parameters: A month, year, and a list of ids. The type hints are specified in the method; the list_of_ids parameter is the particular set of ids found using the developer tools tab in Chrome (in our case, the list_of_ids are [\"ContentPlaceHolder1_rpRailRidership_rpRailSystemwide_gvRailSYS\", \"ContentPlaceHolder1_rpRailRidership_rpRailRed_gvRailRed\", \"ContentPlaceHolder1_rpRailRidership_rpRailBlue_gvRailBlue\",\n",
    "\"ContentPlaceHolder1_rpRailRidership_rpRailExpo_gvRailExpo\",\n",
    "\"ContentPlaceHolder1_rpRailRidership_rpRailGreen_gvRailGreen\",\n",
    "\"ContentPlaceHolder1_rpRailRidership_rpRailGold_gvRailGold\"].\n",
    "\n",
    "We first check to make sure our input is valid (i.e. the Month and Year input is a string, the year is between 2009-2023, and the Month is not November or December for the year 2023). Then we define the selenium webdriver via the webdriver.Chrome() command, and make it look at the base url given earlier.\n",
    "\n",
    "Once here, we need to perform the act of actually clicking on the particular month and year we passed in. We navigate to those boxes via Xpath, and the webdriver has an implemenation allowing us to get to these boxed via the \"find_element\" method. Once here, the webdriver will return all the possible values our \"Month\" and \"Year\" boxes can take on, so we only want to call the \".click()\" method on the specific month/year passed in by the user.\n",
    "\n",
    "Once this is done, we click the \"submit\" button via the same \"find_element\" method for the webdriver. Selenium now loads the new page with the ridership information for that specific month/year, which means we can now use BeautifulSoup to parse this data. At this point, using this soup object, we can call the helper \"parse_ridership\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df98125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_ridership method\n",
    "\n",
    "def parse_ridership(list_of_ids, soup):\n",
    "    \"\"\"\n",
    "    This method takes in a list of the ids corresponding to the ridership of the rail systems on the LA Metro,\n",
    "    and parses them via BeautifulSoup\n",
    "    \"\"\"\n",
    "    dataframes_by_line = []\n",
    "    for table_id in list_of_ids:\n",
    "        rail_ridership = soup.find(lambda tag: tag.name==\"table\" and tag.has_attr('id') and tag['id']==table_id)\n",
    "        rows = defaultdict(list)\n",
    "\n",
    "        # First row is the list of dates\n",
    "        first_row = rail_ridership.find('tr')\n",
    "        dates = first_row.find_all('th')\n",
    "        headers = [\"Boarding Category\"]\n",
    "        headers += [date.text for date in dates if date.text != \"\"]\n",
    "\n",
    "        for row in rail_ridership.find_all('tr'):\n",
    "            for index, tag in enumerate(row.find_all('td')):\n",
    "                rows[headers[index]].append(tag.text)\n",
    "        df = pd.DataFrame(rows)\n",
    "        df.set_index(\"Boarding Category\", drop=True, inplace=True)\n",
    "        dataframes_by_line.append(df)\n",
    "    return dataframes_by_line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59760abd",
   "metadata": {},
   "source": [
    "### Overview of \"parse_ridership\" method\n",
    "\n",
    "Above we have the implementation of the \"parse_ridership\" method. It takes in the list_of_ids we passed into the \"selenium_driver\" method, as well as the soup object created in that method.\n",
    "\n",
    "We first create a list in which we can store the dataframes we get from using BeautifulSoup on the particular set of data generated by the selenium webdriver. Then for each of the ids we have passed in, we get the data within that particular table of the webpage, which is stored in the rail_ridership object. Then for each of the items in rail_ridership, we append the list of dates, the particular category of boarding (i.e. weekday, holiday, total, etc.), and then append the the value present in these sections into the dict named \"rows\" defined earlier.\n",
    "\n",
    "By storing our information in this manner, we can simply convert the dict into a Pandas dataframe for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3462d33a",
   "metadata": {},
   "source": [
    "### Getting relevant data\n",
    "\n",
    "After we have performed this process, we can then begin to think about how to \"weigh\" the lines of the Metro in accordance with what we hope to measure; that is, how delays propogate and can impact other lines. To begin weighing the lines in accordance with their relative importance, we simply consider the total ridership across the metro rail system for the entire month, and then divide out that number with the ridership of each of the lines on the metro to get a proportion (i.e. a number between 0 and 1). We are assisted in this step using the magic of dataframe operations, and as such can simply get the total ridership with the first dataframe in our list, and then divide that number out, row-wise, with the dataframes for each specific line. Once this is done, we can return the new list of dataframes for our purposes of building a network/graph for further analysis for delay propogation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18e1ae",
   "metadata": {},
   "source": [
    "### Running the script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7705d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to run the script\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    list_of_ids = [\"ContentPlaceHolder1_rpRailRidership_rpRailSystemwide_gvRailSYS\",\n",
    "                   \"ContentPlaceHolder1_rpRailRidership_rpRailRed_gvRailRed\",\n",
    "                   \"ContentPlaceHolder1_rpRailRidership_rpRailBlue_gvRailBlue\",\n",
    "                   \"ContentPlaceHolder1_rpRailRidership_rpRailExpo_gvRailExpo\",\n",
    "                   \"ContentPlaceHolder1_rpRailRidership_rpRailGreen_gvRailGreen\",\n",
    "                   \"ContentPlaceHolder1_rpRailRidership_rpRailGold_gvRailGold\"]\n",
    "   \n",
    "    # For our purposes, we will consider data for January - March 2023\n",
    "    # We can enter many months within a given calendar year\n",
    "    months = input(\"Enter month for ridership data: \").split()\n",
    "    year_input = input(\"Enter year for ridership data: \")\n",
    "  \n",
    "    for month in months: \n",
    "        list_of_dataframes = selenium_driver(month, year_input, list_of_ids)\n",
    "        if type(list_of_dataframes) != str:\n",
    "            print(f\"Ridership for {month} {year_input}\")\n",
    "            print(list_of_dataframes)\n",
    "      \n",
    "            for index, dataframe in enumerate(list_of_dataframes):\n",
    "                rail_line = list_of_ids[index].split(\"_\")[-1]\n",
    "                dataframe.to_csv(rail_line+month+year_input+'.csv')\n",
    "        else:\n",
    "            print(list_of_dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdfc7e9",
   "metadata": {},
   "source": [
    "For our purposes, we decided that we just needed to run the script via the command line. The way the script is currently set up, we can enter a list of months seperated by a space which will be read by the script as the frist part of the input, and a single year for the \"year\" parameter. We then call the \"selenium_driver\" method, and then read the data returned out to a csv file in the same directory as a script.\n",
    "\n",
    "One thing to note would be that the current implementation involes the relaunching of the selenium webdriver each time we need to enter a new month/year combination. This is because we encountered an error with the webdriver being unable to render the page to apply BeautifulSoup to actually parse the data we need. As such, we decided that since we were only parsing a few months' data, it would be sufficient to simply get data for each month at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5babedb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
